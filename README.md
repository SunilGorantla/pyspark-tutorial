# pyspark questions to be answered.
* What are UDFs and why are they important
* MLlib functionalities
* File systems used in spark
* Main Characteristic of Spark (Node abstraction, MapReduce, APIs for spark functions, Abstracted Networks)
* PySpark Serializers
* Custome profilers
* Pyspark StorageLevel
* Memory Optimization Techniques
* Garbage collection techniques
* Memory Usage of Reduce tasks
* Broadcasting large variables
* Locality
* Use dataframe over RDD (Dataframe does not need serialization, includes catalyst and project tungsten)
* Use coleasce instead of repartition
* Use mapPartitions instead of map
* Use serialized data formats (Parquet, Avro)
* Avoid UDF's
* Persisting & Caching data in memory
* Reduce Expensive shuffle operations
* Disable debug and info logging
* Avoid For and While loops as much as possible
